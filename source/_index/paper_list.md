# Paper list

> The paper list is not comprehensive, and 
> the prior work is divided into several categories based on personal understanding.

## Master Branch for Me

### 2015-2016

1.  Deep learning with limited numerical precision. **2015 IBM**
2.  **DoReFa-Net**: Training low bit-width convolutional neural networks with low bit-width gradients. **2016**
4.  **BNN**: Binarized Neural Networks. **NIPS2016**
7.  **TWNs**: Ternary weight networks. **NIPS2016** ucas
3.  **XNOR-Net**: ImageNet Classification using binary convolutional neural networks. **ECCV2016 washington**
6.  Hardware-oriented approximation of convolutional neural networks. **ICLR2016**
8.  Quantized convolutional neural networks for mobile devices. **CVPR2016** nlpr

### 2017
9.  **Flexpoint**: an adaptive numerical format for efficient training of deep neural networks. **2017** intel
10. **INQ**: Incremental network quantization, towards lossless CNNs with low-precision weights. **ICLR2017** intel labs china
11. **TTQ**: Trained ternary quantization. **ICLR2017** stanford
12. **WRPN**: wide reduced-precision networks. **2017** Accelerator Architecture Lab, Intel
14. **HWGQ**: Deep Learning with Low Precision by Half-wave Gaussian Quantization. **CVPR2017**
13. A **Survey** of Model Compression and Acceleration for Deep Neural Networks. **2017**


### 2018

#### ICLR2018
15. **VNQ**: Variational network quantization. **ICLR2018**
16. **WAGE**: Training and Inference with Integers in Deep Neural Networks. **ICLR2018** oral tsinghua
21. Alternating multi-bit quantization for recurrent neural networks. **ICLR2018** alibaba
28. Mixed Precision Training. **ICLR2018** baidu
30. Model Compression via distillation and quantization. **ICLR2018** google
33. Quantized back-propagation: training binarized neural networks with quantized gradients. **ICLR2018**

#### CVPR2018
17. **Clip-Q**: Deep network compression learning by In-Parallel Pruning Quantization. **CVPR2018** SFU
23. **ELQ**: Explicit loss-error-aware quantization for low-bit deep neural networks. **CVPR2018** intel tsinghua
31. Quantization and training of neural networks for efficient integer-arithmetic-only inference. **CVPR2018** Google
37. **TSQ**: two-step quantization for low-bit neural networks. **CVPR2018**
36. **SYQ**: learning symmetric quantization for efficient deep neural networks. **CVPR2018** xilinx
32. Towards Effective Low-bitwidth Convolutional Neural Networks. **CVPR2018**

#### ECCV2018
18. **LQ-NETs**: learned quantization for highly accurate and compact deep neural networks. **ECCV2018** Microsoft
19. **Bi-Real Net**: Enhancing the performance of 1-bit CNNs with improved Representational capability and advanced training algorithm. **ECCV2018** HKU
38. **V-Quant**: Value-aware quantization for training and inference of neural networks. **ECCV2018** facebook

#### NIPS2018
26. Heterogeneous Bitwidth Binarization in Convolutional Neural Networks. **NIPS2018** microsoft
25. **HAQ**: Hardware-Aware automated quantization. **NIPS workshop 2018** mit
35. Scalable methods for 8-bits training of neural networks. **NIPS2018** intel

#### AAAI2018
24. From Hashing to CNNs: training Binary weights vis hashing. **AAAI2018** nlpr

#### Other
20. **Synergy**: Algorithm-hardware co-design for convnet accelerators on embedded FPGAs. **2018** UC Berkeley
22. Efficient Non-uniform quantizer for quantized neural network targeting Re-configurable hardware. **2018**
27. **HALP**: High-Accuracy Low-Precision Training. **2018** stanford
29. **PACT**: parameterized clipping activation for quantized neural networks. **2018** IBM
34. **QUENN**: Quantization engine for low-power neural networks. **CF18ACM**
39. **UNIQ**: Uniform noise injection for non-uniform quantization of neural networks. **2018**
41. Training competitive binary neural networks from scratch. **2018**
42. **A white-paper**: Quantizing deep convolutional networks for efficient inference. **2018** google


### 2019
====ICLR

43. **ACIQ**: analytical clipping for integer quantization of neural networks. **ICLR2019** Intel
44. Per-Tensor Fixed-point quantization of the back-propagation algorithm. **ICLR2019**
45. **RQ**: Relaxed Quantization for disretized NNs. **ICLR2019**

==== NIPS

49. **Post training** 4-bit quantization of convolution networks for rapid-deployment. **NIPS 2019** AIPG, Intel

==== ICCV

50. **DSQ**: Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks. **ICCV2019** SenseTime, Beihang

==== CVPR

50. **FQN**: Fully Quantized Network for Object Detection. **CVPR2019**
53. **QIL**: Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss **CVPR2019**
==== Other
46. **SAWB**: Accurate and efficient 2-bit quantized neural networks. **sysml2019**
47. **SQuantizer**: Simultaneous Learning for Both Sparse and Low-precision Neural Networks. **2019** AIPG, Intel

### 2020
51. **LSQ**: Learned Step Size Quantization **ICLR2020 open-review**
52. Mixed Precision DNNs: All you need is a good parametrization **ICLR2020 open-review**
54. **HAWQv2**: Hessian Aware trace-Weighted Quantization of Neural Networks

## Nonclear for Me
5.  Fixed point quantization of deep convolutional networks. **2016**
40. Training a binary weight object detector by knowledge transfer for autonomous driving. **2018**
48. Low-bit Quantization of Neural Networks for EfÔ¨Åcient Inference. **2019** huawei



