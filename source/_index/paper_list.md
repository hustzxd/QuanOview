# Paper list

1.  Deep learning with limited numerical precision. **2015 IBM**
2.  **DoReFa-Net**: Training low bit-width convolutional neural networks with low bit-width gradients. **2016**
3.  **XNOR-Net**: ImageNet Classification using binary convolutional
    neural networks. **ECCV2016 washington**
4.  **BNN**: Binarized Neural Networks. **NIPS2016**
5.  Fixed point quantization of deep convolutional networks. **2016**
6.  Hardware-oriented approximation of convolutional neural networks.
    **ICLR2016**
7.  **TWNs**: Ternary weight networks. **NIPS2016** ucas
8.  Quantized convolutional neural networks for mobile devices.
    **CVPR2016** nlpr
9.  **Flexpoint**: an adaptive numerical format for efficient training
    of deep neural networks. **2017** intel
10. **INQ**: Incremental network quantization, towards lossless CNNs
    with low-precision weights. **ICLR2017** intel labs china
11. **TTQ**: Trained ternary quantization. **ICLR2017** stanford
12. **WRPN**: wide reduced-precision networks. **2017**
13. A Survey of Model Compression and Acceleration for Deep Neural
    Networks. **201712**
14. **HWGQ**: Deep Learning with Low Precision by Half-wave Gaussian
    Quantization. **CVPR2017**
15. **VNQ**: Variational network quantization. **ICLR2018**
16. **WAGE**: Training and Inference with Integers in Deep Neural
    Networks. **ICLR2018** oral tsinghua
17. **Clip-Q**: Deep network compression learning by In-Parallel Pruning
    Quantization. **CVPR2018** SFU
18. **LQ-NETs**: learned quantization for highly accurate and compact
    deep neural networks. **ECCV2018** Microsoft
19. **Bi-Real Net**: Enhancing the performance of 1-bit CNNs with
    improved Representational capability and advanced training
    algorithm. **ECCV2018** HKU
20. **Synergy**: Algorithm-hardware co-design for convnet accelerators
    on embedded FPGAs. **2018** UC Berkeley
21. Alternating multi-bit quantization for recurrent neural networks.
    **ICLR2018** alibaba
22. Efficient Non-uniform quantizer for quantized neural network
    targeting Re-configurable hardware. **2018**
23. **ELQ**: Explicit loss-error-aware quantization for low-bit deep
    neural networks. **CVPR2018** intel tsinghua
24. From Hashing to CNNs: training Binary weights vis hashing.
    **AAAI2018** nlpr
25. **HAQ**: Hardware-Aware automated quantization. **NIPS
    workshop2018** mit
26. Heterogeneous Bitwidth Binarization in Convolutional Neural
    Networks. **NIPS2018** microsoft
27. **HALP**: High-Accuracy Low-Precision Training. **2018** stanford
28. Mixed Precision Training. **ICLR2018** baidu
29. **PACT**: parameterized clipping activation for quantized neural
    networks. **2018** IBM
30. Model Compression via distillation and quantization. **ICLR2018**
    google
31. Quantization and training of neural networks for efficient
    integer-arithmetic-only inference. **CVPR2018** Google
32. Towards Effective Low-bitwidth Convolutional Neural Networks.
    **CVPR2018**
33. Quantized back-propagation: training binarized neural networks with
    quantized gradients. **ICLR2018**
34. **QUENN**: Quantization engine for low-power neural networks. **CF18
    ACM**
35. Scalable methods for 8-bits training of neural networks.
    **NIPS2018** intel
36. **SYQ**: learning symmetric quantization for efficient deep neural
    networks. **CVPR2018** xilinx
37. **TSQ**: two-step quantization for low-bit neural networks. **CVPR2018**
38. **V-Quant**: Value-aware quantization for training and inference of
    neural networks. **ECCV2018** facebook
39. **UNIQ**: Uniform noise injection for non-uniform quantization of
    neural networks. **2018**
40. Training a binary weight object detector by knowledge transfer for
    autonomous driving. **2018**
41. Training competitive binary neural networks from scratch. **2018**
42. **A white-paper**: Quantizing deep convolutional networks for
    efficient inference. **2018** google
43. **ACIQ**: analytical clipping for integer quantization of neural
    networks. **ICLR2019** Intel
44. Per-Tensor Fixed-point quantization of the back-propagation
    algorithm. **ICLR2019**
45. **RQ**: Relaxed Quantization for disretized NNs. **ICLR2019**
46. **SAWB**: Accurate and efficient 2-bit quantized neural networks.
    **sysml2019**
47. **SQuantizer**: Simultaneous Learning for Both Sparse and
    Low-precision Neural Networks. **2019** AIPG, Intel
48. Low-bit Quantization of Neural Networks for EfÔ¨Åcient Inference.
    **2019** huawei
49. **Post training** 4-bit quantization of convolution networks for
    rapid-deployment. **2019**
50. **FQN**: Fully Quantized Network for Object Detection. **CVPR2019**
51. **LSQ**: Learned Step Size Quantization **ICLR2020**
52. Mixed Precision DNNs: All you need is a good parametrization **ICLR2020 open review**
53. **QIL**: Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss
54. **HAWQv2**: Hessian Aware trace-Weighted Quantization of Neural Networks

## Not importan


